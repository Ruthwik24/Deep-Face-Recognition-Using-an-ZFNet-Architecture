{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1kP15PnVtnkI-S6GrJ59vggJMrI49vPZ4",
      "authorship_tag": "ABX9TyPQc16Am2mHEmxRvTbhgXyj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ruthwik24/Deep-Face-Recognition-Using-an-ZFNet-Architecture/blob/main/Deep_Face_Recognition_Using_an_ZFNet_Architecture.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DV6UFQMDbpqj",
        "outputId": "2e307920-b63a-4749-b348-e7b69f88e62a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HbyMQPgpbTsZ",
        "outputId": "cb43d41e-cffe-4090-8791-d171c9ff21ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converted and saved: /content/drive/MyDrive/Preprocessed/subject02/normal.jpg\n",
            "Converted and saved: /content/drive/MyDrive/Preprocessed/subject01/glasses.jpg\n",
            "Converted and saved: /content/drive/MyDrive/Preprocessed/subject01/normal.jpg\n",
            "Converted and saved: /content/drive/MyDrive/Preprocessed/subject01/rightlight.jpg\n",
            "Converted and saved: /content/drive/MyDrive/Preprocessed/subject01/sleepy.jpg\n",
            "Converted and saved: /content/drive/MyDrive/Preprocessed/subject01/leftlight.jpg\n",
            "Converted and saved: /content/drive/MyDrive/Preprocessed/subject01/wink.jpg\n",
            "Converted and saved: /content/drive/MyDrive/Preprocessed/subject02/glasses.jpg\n",
            "Converted and saved: /content/drive/MyDrive/Preprocessed/subject01/noglasses.jpg\n",
            "Converted and saved: /content/drive/MyDrive/Preprocessed/subject01/sad.jpg\n",
            "Converted and saved: /content/drive/MyDrive/Preprocessed/subject02/noglasses.jpg\n",
            "Converted and saved: /content/drive/MyDrive/Preprocessed/subject02/happy.jpg\n",
            "Converted and saved: /content/drive/MyDrive/Preprocessed/subject01/surprised.jpg\n",
            "Converted and saved: /content/drive/MyDrive/Preprocessed/subject01/happy.jpg\n",
            "Converted and saved: /content/drive/MyDrive/Preprocessed/subject02/leftlight.jpg\n",
            "Converted and saved: /content/drive/MyDrive/Preprocessed/subject02/centerlight.jpg\n",
            "Converted and saved: /content/drive/MyDrive/Preprocessed/subject01/centerlight.jpg\n",
            "Converted and saved: /content/drive/MyDrive/Preprocessed/subject05/noglasses.jpg\n",
            "Converted and saved: /content/drive/MyDrive/Preprocessed/subject03/happy.jpg\n",
            "Converted and saved: /content/drive/MyDrive/Preprocessed/subject02/surprised.jpg\n",
            "Converted and saved: /content/drive/MyDrive/Preprocessed/subject06/happy.jpg\n",
            "Converted and saved: /content/drive/MyDrive/Preprocessed/subject02/wink.jpg\n",
            "Converted and saved: /content/drive/MyDrive/Preprocessed/subject05/sad.jpg\n",
            "Converted and saved: /content/drive/MyDrive/Preprocessed/subject04/leftlight.jpg\n",
            "Converted and saved: /content/drive/MyDrive/Preprocessed/subject04/noglasses.jpg\n",
            "Converted and saved: /content/drive/MyDrive/Preprocessed/subject05/sleepy.jpg\n",
            "Converted and saved: /content/drive/MyDrive/Preprocessed/subject05/glasses.jpg\n",
            "Converted and saved: /content/drive/MyDrive/Preprocessed/subject05/surprised.jpg\n",
            "Converted and saved: /content/drive/MyDrive/Preprocessed/subject03/leftlight.jpg\n",
            "Converted and saved: /content/drive/MyDrive/Preprocessed/subject02/rightlight.jpg\n",
            "Converted and saved: /content/drive/MyDrive/Preprocessed/subject03/rightlight.jpg\n",
            "Converted and saved: /content/drive/MyDrive/Preprocessed/subject05/happy.jpg\n",
            "Converted and saved: /content/drive/MyDrive/Preprocessed/subject06/noglasses.jpg\n",
            "Converted and saved: /content/drive/MyDrive/Preprocessed/subject03/sleepy.jpg\n",
            "Converted and saved: /content/drive/MyDrive/Preprocessed/subject06/glasses.jpg\n",
            "Converted and saved: /content/drive/MyDrive/Preprocessed/subject04/wink.jpg\n",
            "Converted and saved: /content/drive/MyDrive/Preprocessed/subject03/normal.jpg\n",
            "Converted and saved: /content/drive/MyDrive/Preprocessed/subject03/wink.jpg\n",
            "Converted and saved: /content/drive/MyDrive/Preprocessed/subject06/leftlight.jpg\n",
            "Converted and saved: /content/drive/MyDrive/Preprocessed/subject02/sad.jpg\n",
            "Converted and saved: /content/drive/MyDrive/Preprocessed/subject06/centerlight.jpg\n",
            "Converted and saved: /content/drive/MyDrive/Preprocessed/subject05/normal.jpg\n",
            "Converted and saved: /content/drive/MyDrive/Preprocessed/subject04/sleepy.jpg\n",
            "Converted and saved: /content/drive/MyDrive/Preprocessed/subject05/leftlight.jpg\n",
            "Converted and saved: /content/drive/MyDrive/Preprocessed/subject03/sad.jpg\n",
            "Converted and saved: /content/drive/MyDrive/Preprocessed/subject04/happy.jpg\n",
            "Converted and saved: /content/drive/MyDrive/Preprocessed/subject04/centerlight.jpg\n",
            "Converted and saved: /content/drive/MyDrive/Preprocessed/subject03/noglasses.jpg\n",
            "Converted and saved: /content/drive/MyDrive/Preprocessed/subject06/normal.jpg\n",
            "Converted and saved: /content/drive/MyDrive/Preprocessed/subject04/rightlight.jpg\n",
            "Converted and saved: /content/drive/MyDrive/Preprocessed/subject03/centerlight.jpg\n",
            "Converted and saved: /content/drive/MyDrive/Preprocessed/subject05/wink.jpg\n",
            "Converted and saved: /content/drive/MyDrive/Preprocessed/subject05/centerlight.jpg\n",
            "Converted and saved: /content/drive/MyDrive/Preprocessed/subject02/sleepy.jpg\n",
            "Converted and saved: /content/drive/MyDrive/Preprocessed/subject04/sad.jpg\n",
            "Converted and saved: /content/drive/MyDrive/Preprocessed/subject04/surprised.jpg\n",
            "Converted and saved: /content/drive/MyDrive/Preprocessed/subject03/surprised.jpg\n",
            "Converted and saved: /content/drive/MyDrive/Preprocessed/subject04/glasses.jpg\n",
            "Converted and saved: /content/drive/MyDrive/Preprocessed/subject05/rightlight.jpg\n",
            "Converted and saved: /content/drive/MyDrive/Preprocessed/subject03/glasses.jpg\n",
            "Converted and saved: /content/drive/MyDrive/Preprocessed/subject04/normal.jpg\n",
            "Converted and saved: /content/drive/MyDrive/Preprocessed/subject09/surprised.jpg\n",
            "Converted and saved: /content/drive/MyDrive/Preprocessed/subject10/sad.jpg\n",
            "Converted and saved: /content/drive/MyDrive/Preprocessed/subject08/happy.jpg\n",
            "Converted and saved: /content/drive/MyDrive/Preprocessed/subject07/rightlight.jpg\n",
            "Converted and saved: /content/drive/MyDrive/Preprocessed/subject08/wink.jpg\n",
            "Converted and saved: /content/drive/MyDrive/Preprocessed/subject07/surprised.jpg\n",
            "Converted and saved: /content/drive/MyDrive/Preprocessed/subject06/rightlight.jpg\n",
            "Converted and saved: /content/drive/MyDrive/Preprocessed/subject08/surprised.jpg\n",
            "Converted and saved: /content/drive/MyDrive/Preprocessed/subject07/noglasses.jpg\n",
            "Converted and saved: /content/drive/MyDrive/Preprocessed/subject09/normal.jpg\n",
            "Converted and saved: /content/drive/MyDrive/Preprocessed/subject10/normal.jpg\n",
            "Converted and saved: /content/drive/MyDrive/Preprocessed/subject08/centerlight.jpg\n",
            "Converted and saved: /content/drive/MyDrive/Preprocessed/subject09/wink.jpg\n",
            "Converted and saved: /content/drive/MyDrive/Preprocessed/subject08/noglasses.jpg\n",
            "Converted and saved: /content/drive/MyDrive/Preprocessed/subject07/happy.jpg\n",
            "Converted and saved: /content/drive/MyDrive/Preprocessed/subject08/glasses.jpg\n",
            "Converted and saved: /content/drive/MyDrive/Preprocessed/subject10/happy.jpg\n",
            "Converted and saved: /content/drive/MyDrive/Preprocessed/subject09/leftlight.jpg\n",
            "Converted and saved: /content/drive/MyDrive/Preprocessed/subject06/surprised.jpg\n",
            "Converted and saved: /content/drive/MyDrive/Preprocessed/subject08/rightlight.jpg\n",
            "Converted and saved: /content/drive/MyDrive/Preprocessed/subject07/normal.jpg\n",
            "Converted and saved: /content/drive/MyDrive/Preprocessed/subject07/leftlight.jpg\n",
            "Converted and saved: /content/drive/MyDrive/Preprocessed/subject07/centerlight.jpg\n",
            "Converted and saved: /content/drive/MyDrive/Preprocessed/subject10/centerlight.jpg\n",
            "Converted and saved: /content/drive/MyDrive/Preprocessed/subject10/rightlight.jpg\n",
            "Converted and saved: /content/drive/MyDrive/Preprocessed/subject08/sleepy.jpg\n",
            "Converted and saved: /content/drive/MyDrive/Preprocessed/subject09/centerlight.jpg\n",
            "Converted and saved: /content/drive/MyDrive/Preprocessed/subject06/wink.jpg\n",
            "Converted and saved: /content/drive/MyDrive/Preprocessed/subject07/glasses.jpg\n",
            "Converted and saved: /content/drive/MyDrive/Preprocessed/subject09/sad.jpg\n",
            "Converted and saved: /content/drive/MyDrive/Preprocessed/subject10/leftlight.jpg\n",
            "Converted and saved: /content/drive/MyDrive/Preprocessed/subject09/happy.jpg\n",
            "Converted and saved: /content/drive/MyDrive/Preprocessed/subject08/sad.jpg\n",
            "Converted and saved: /content/drive/MyDrive/Preprocessed/subject06/sleepy.jpg\n",
            "Converted and saved: /content/drive/MyDrive/Preprocessed/subject06/sad.jpg\n",
            "Converted and saved: /content/drive/MyDrive/Preprocessed/subject09/sleepy.jpg\n",
            "Converted and saved: /content/drive/MyDrive/Preprocessed/subject10/glasses.jpg\n",
            "Converted and saved: /content/drive/MyDrive/Preprocessed/subject08/leftlight.jpg\n",
            "Converted and saved: /content/drive/MyDrive/Preprocessed/subject08/normal.jpg\n",
            "Converted and saved: /content/drive/MyDrive/Preprocessed/subject09/glasses.jpg\n",
            "Converted and saved: /content/drive/MyDrive/Preprocessed/subject09/rightlight.jpg\n",
            "Converted and saved: /content/drive/MyDrive/Preprocessed/subject09/noglasses.jpg\n",
            "Converted and saved: /content/drive/MyDrive/Preprocessed/subject07/sleepy.jpg\n",
            "Converted and saved: /content/drive/MyDrive/Preprocessed/subject07/sad.jpg\n",
            "Converted and saved: /content/drive/MyDrive/Preprocessed/subject10/noglasses.jpg\n",
            "Converted and saved: /content/drive/MyDrive/Preprocessed/subject07/wink.jpg\n",
            "Converted and saved: /content/drive/MyDrive/Preprocessed/subject13/normal.jpg\n",
            "Converted and saved: /content/drive/MyDrive/Preprocessed/subject14/glasses.jpg\n",
            "Converted and saved: /content/drive/MyDrive/Preprocessed/subject11/sleepy.jpg\n",
            "Converted and saved: /content/drive/MyDrive/Preprocessed/subject12/happy.jpg\n",
            "Converted and saved: /content/drive/MyDrive/Preprocessed/subject14/rightlight.jpg\n",
            "Converted and saved: /content/drive/MyDrive/Preprocessed/subject11/leftlight.jpg\n",
            "Converted and saved: /content/drive/MyDrive/Preprocessed/subject12/leftlight.jpg\n",
            "Converted and saved: /content/drive/MyDrive/Preprocessed/subject13/sleepy.jpg\n",
            "Converted and saved: /content/drive/MyDrive/Preprocessed/subject13/sad.jpg\n",
            "Converted and saved: /content/drive/MyDrive/Preprocessed/subject13/wink.jpg\n",
            "Converted and saved: /content/drive/MyDrive/Preprocessed/subject12/normal.jpg\n",
            "Converted and saved: /content/drive/MyDrive/Preprocessed/subject12/rightlight.jpg\n",
            "Converted and saved: /content/drive/MyDrive/Preprocessed/subject12/surprised.jpg\n",
            "Converted and saved: /content/drive/MyDrive/Preprocessed/subject11/rightlight.jpg\n",
            "Converted and saved: /content/drive/MyDrive/Preprocessed/subject11/sad.jpg\n",
            "Converted and saved: /content/drive/MyDrive/Preprocessed/subject13/happy.jpg\n",
            "Converted and saved: /content/drive/MyDrive/Preprocessed/subject10/sleepy.jpg\n",
            "Converted and saved: /content/drive/MyDrive/Preprocessed/subject14/leftlight.jpg\n",
            "Converted and saved: /content/drive/MyDrive/Preprocessed/subject14/sad.jpg\n",
            "Converted and saved: /content/drive/MyDrive/Preprocessed/subject13/noglasses.jpg\n",
            "Converted and saved: /content/drive/MyDrive/Preprocessed/subject11/centerlight.jpg\n",
            "Converted and saved: /content/drive/MyDrive/Preprocessed/subject12/wink.jpg\n",
            "Converted and saved: /content/drive/MyDrive/Preprocessed/subject14/normal.jpg\n",
            "Converted and saved: /content/drive/MyDrive/Preprocessed/subject11/normal.jpg\n",
            "Converted and saved: /content/drive/MyDrive/Preprocessed/subject11/surprised.jpg\n",
            "Converted and saved: /content/drive/MyDrive/Preprocessed/subject13/surprised.jpg\n",
            "Converted and saved: /content/drive/MyDrive/Preprocessed/subject13/leftlight.jpg\n",
            "Converted and saved: /content/drive/MyDrive/Preprocessed/subject13/rightlight.jpg\n",
            "Converted and saved: /content/drive/MyDrive/Preprocessed/subject11/noglasses.jpg\n",
            "Converted and saved: /content/drive/MyDrive/Preprocessed/subject12/noglasses.jpg\n",
            "Converted and saved: /content/drive/MyDrive/Preprocessed/subject14/noglasses.jpg\n",
            "Converted and saved: /content/drive/MyDrive/Preprocessed/subject14/happy.jpg\n",
            "Converted and saved: /content/drive/MyDrive/Preprocessed/subject12/sleepy.jpg\n",
            "Converted and saved: /content/drive/MyDrive/Preprocessed/subject11/glasses.jpg\n",
            "Converted and saved: /content/drive/MyDrive/Preprocessed/subject13/glasses.jpg\n",
            "Converted and saved: /content/drive/MyDrive/Preprocessed/subject14/sleepy.jpg\n",
            "Converted and saved: /content/drive/MyDrive/Preprocessed/subject10/surprised.jpg\n",
            "Converted and saved: /content/drive/MyDrive/Preprocessed/subject11/happy.jpg\n",
            "Converted and saved: /content/drive/MyDrive/Preprocessed/subject13/centerlight.jpg\n",
            "Converted and saved: /content/drive/MyDrive/Preprocessed/subject12/sad.jpg\n",
            "Converted and saved: /content/drive/MyDrive/Preprocessed/subject12/centerlight.jpg\n",
            "Converted and saved: /content/drive/MyDrive/Preprocessed/subject14/centerlight.jpg\n",
            "Converted and saved: /content/drive/MyDrive/Preprocessed/subject12/glasses.jpg\n",
            "Converted and saved: /content/drive/MyDrive/Preprocessed/subject10/wink.jpg\n",
            "Converted and saved: /content/drive/MyDrive/Preprocessed/subject11/wink.jpg\n",
            "Converted and saved: /content/drive/MyDrive/Preprocessed/subject15/surprised.jpg\n",
            "Converted and saved: /content/drive/MyDrive/Preprocessed/subject15/sleepy.jpg\n",
            "Converted and saved: /content/drive/MyDrive/Preprocessed/subject15/wink.jpg\n",
            "Converted and saved: /content/drive/MyDrive/Preprocessed/subject15/leftlight.jpg\n",
            "Converted and saved: /content/drive/MyDrive/Preprocessed/subject15/sad.jpg\n",
            "Converted and saved: /content/drive/MyDrive/Preprocessed/subject15/rightlight.jpg\n",
            "Converted and saved: /content/drive/MyDrive/Preprocessed/subject14/surprised.jpg\n",
            "Converted and saved: /content/drive/MyDrive/Preprocessed/subject15/normal.jpg\n",
            "Converted and saved: /content/drive/MyDrive/Preprocessed/subject14/wink.jpg\n",
            "Converted and saved: /content/drive/MyDrive/Preprocessed/subject15/noglasses.jpg\n",
            "Converted and saved: /content/drive/MyDrive/Preprocessed/subject15/happy.jpg\n",
            "Converted and saved: /content/drive/MyDrive/Preprocessed/subject15/glasses.jpg\n",
            "Converted and saved: /content/drive/MyDrive/Preprocessed/subject15/centerlight.jpg\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "# Source folder where the images are currently located\n",
        "source_folder = r'/content/drive/MyDrive/archive'\n",
        "\n",
        "# Target folder where we will organize the images\n",
        "processed_data_folder = r'/content/drive/MyDrive/Preprocessed'\n",
        "\n",
        "# Check if the source folder exists\n",
        "if not os.path.exists(source_folder):\n",
        "    print(f\"Source folder '{source_folder}' does not exist.\")\n",
        "else:\n",
        "    # Create the processed folder if it doesn't exist\n",
        "    if not os.path.exists(processed_data_folder):\n",
        "        os.makedirs(processed_data_folder)\n",
        "\n",
        "    # Process each file in the source folder\n",
        "    for filename in os.listdir(source_folder):\n",
        "        # Ignore non-image files (e.g., Readme)\n",
        "        if not filename.startswith(\"subject\") or len(filename.split('.')) < 2:\n",
        "            continue\n",
        "\n",
        "        # Extract subject and expression from the filename\n",
        "        parts = filename.split('.')\n",
        "        subject = parts[0]  # e.g., \"subject01\"\n",
        "        expression = parts[1]  # e.g., \"centerlight\"\n",
        "\n",
        "        # Create a folder for each subject if it doesn't exist\n",
        "        subject_folder = os.path.join(processed_data_folder, subject)\n",
        "        if not os.path.exists(subject_folder):\n",
        "            os.makedirs(subject_folder)\n",
        "\n",
        "        # Define the destination path for the converted image\n",
        "        dst_path = os.path.join(subject_folder, f\"{expression}.jpg\")\n",
        "\n",
        "        # Convert and save the image to JPEG format\n",
        "        src_path = os.path.join(source_folder, filename)\n",
        "        try:\n",
        "            with Image.open(src_path) as img:\n",
        "                img = img.convert('RGB')  # Convert to RGB format if needed\n",
        "                img.save(dst_path, 'JPEG')\n",
        "                print(f\"Converted and saved: {dst_path}\")  # Print each conversion for confirmation\n",
        "        except Exception as e:\n",
        "            print(f\"Could not process file {filename}: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
        "\n",
        "# ZFNet-Inspired Basic Model\n",
        "def build_basic_zfnet_model(input_shape, num_classes):\n",
        "    model = models.Sequential()\n",
        "\n",
        "    # First Convolutional Block\n",
        "    model.add(layers.Conv2D(48, (7, 7), strides=(2, 2), activation='relu', input_shape=input_shape))\n",
        "    model.add(layers.MaxPooling2D((2, 2)))\n",
        "\n",
        "    # Second Convolutional Block\n",
        "    model.add(layers.Conv2D(128, (5, 5), strides=(2, 2), activation='relu'))\n",
        "    model.add(layers.MaxPooling2D((2, 2)))\n",
        "\n",
        "    # Third Convolutional Block\n",
        "    model.add(layers.Conv2D(192, (3, 3), activation='relu'))\n",
        "    model.add(layers.MaxPooling2D((2, 2)))\n",
        "\n",
        "    # Fully Connected Layers\n",
        "    model.add(layers.Flatten())\n",
        "    model.add(layers.Dense(64, activation='relu'))  # Simplified dense layer size\n",
        "    model.add(layers.Dropout(0.5))\n",
        "    model.add(layers.Dense(num_classes, activation='softmax'))\n",
        "\n",
        "    return model\n",
        "\n",
        "# Set up data generators\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    validation_split=0.2\n",
        ")\n",
        "\n",
        "input_shape = (224, 224, 3)\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    '/content/drive/MyDrive/Preprocessed',\n",
        "    target_size=(224, 224),\n",
        "    batch_size=16,\n",
        "    class_mode='categorical',\n",
        "    subset='training'\n",
        ")\n",
        "\n",
        "val_generator = train_datagen.flow_from_directory(\n",
        "    '/content/drive/MyDrive/Preprocessed',\n",
        "    target_size=(224, 224),\n",
        "    batch_size=16,\n",
        "    class_mode='categorical',\n",
        "    subset='validation'\n",
        ")\n",
        "\n",
        "# Initialize and compile the ZFNet-inspired model\n",
        "num_classes = train_generator.num_classes\n",
        "model = build_basic_zfnet_model(input_shape, num_classes)\n",
        "model.compile(optimizer=Adam(learning_rate=1e-4), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Set up callbacks\n",
        "lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6)\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=6, restore_best_weights=True)\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    validation_data=val_generator,\n",
        "    epochs=45,\n",
        "    callbacks=[lr_scheduler, early_stopping]\n",
        ")\n",
        "\n",
        "# Evaluate the model\n",
        "val_loss, val_accuracy = model.evaluate(val_generator)\n",
        "print(f\"Validation accuracy: {val_accuracy * 100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HmNRwYDQbmjc",
        "outputId": "4331c182-7a29-411f-e288-940829e03bce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 135 images belonging to 15 classes.\n",
            "Found 30 images belonging to 15 classes.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/45\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 876ms/step - accuracy: 0.0872 - loss: 2.7172 - val_accuracy: 0.0667 - val_loss: 2.6852 - learning_rate: 1.0000e-04\n",
            "Epoch 2/45\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 544ms/step - accuracy: 0.1245 - loss: 2.6764 - val_accuracy: 0.1667 - val_loss: 2.6649 - learning_rate: 1.0000e-04\n",
            "Epoch 3/45\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 837ms/step - accuracy: 0.2108 - loss: 2.6310 - val_accuracy: 0.2333 - val_loss: 2.6240 - learning_rate: 1.0000e-04\n",
            "Epoch 4/45\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 537ms/step - accuracy: 0.1910 - loss: 2.5973 - val_accuracy: 0.3333 - val_loss: 2.5756 - learning_rate: 1.0000e-04\n",
            "Epoch 5/45\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 846ms/step - accuracy: 0.2256 - loss: 2.5556 - val_accuracy: 0.3333 - val_loss: 2.5078 - learning_rate: 1.0000e-04\n",
            "Epoch 6/45\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 546ms/step - accuracy: 0.2898 - loss: 2.4672 - val_accuracy: 0.3333 - val_loss: 2.4093 - learning_rate: 1.0000e-04\n",
            "Epoch 7/45\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 694ms/step - accuracy: 0.2192 - loss: 2.4379 - val_accuracy: 0.4333 - val_loss: 2.2898 - learning_rate: 1.0000e-04\n",
            "Epoch 8/45\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 553ms/step - accuracy: 0.2337 - loss: 2.3858 - val_accuracy: 0.3000 - val_loss: 2.1723 - learning_rate: 1.0000e-04\n",
            "Epoch 9/45\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 545ms/step - accuracy: 0.2724 - loss: 2.1892 - val_accuracy: 0.5333 - val_loss: 2.0112 - learning_rate: 1.0000e-04\n",
            "Epoch 10/45\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 675ms/step - accuracy: 0.4098 - loss: 1.9624 - val_accuracy: 0.4667 - val_loss: 1.8402 - learning_rate: 1.0000e-04\n",
            "Epoch 11/45\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 619ms/step - accuracy: 0.3662 - loss: 1.8722 - val_accuracy: 0.5667 - val_loss: 1.6923 - learning_rate: 1.0000e-04\n",
            "Epoch 12/45\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 1s/step - accuracy: 0.4648 - loss: 1.8241 - val_accuracy: 0.6000 - val_loss: 1.5850 - learning_rate: 1.0000e-04\n",
            "Epoch 13/45\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 729ms/step - accuracy: 0.4187 - loss: 1.6820 - val_accuracy: 0.6000 - val_loss: 1.4841 - learning_rate: 1.0000e-04\n",
            "Epoch 14/45\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 623ms/step - accuracy: 0.5619 - loss: 1.5320 - val_accuracy: 0.7667 - val_loss: 1.3691 - learning_rate: 1.0000e-04\n",
            "Epoch 15/45\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 544ms/step - accuracy: 0.5422 - loss: 1.3709 - val_accuracy: 0.7333 - val_loss: 1.1510 - learning_rate: 1.0000e-04\n",
            "Epoch 16/45\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 914ms/step - accuracy: 0.5342 - loss: 1.3422 - val_accuracy: 0.7667 - val_loss: 1.1555 - learning_rate: 1.0000e-04\n",
            "Epoch 17/45\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 561ms/step - accuracy: 0.6181 - loss: 1.1794 - val_accuracy: 0.7667 - val_loss: 1.0650 - learning_rate: 1.0000e-04\n",
            "Epoch 18/45\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 741ms/step - accuracy: 0.5675 - loss: 1.3155 - val_accuracy: 0.7667 - val_loss: 1.0240 - learning_rate: 1.0000e-04\n",
            "Epoch 19/45\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 560ms/step - accuracy: 0.6792 - loss: 1.0477 - val_accuracy: 0.8333 - val_loss: 0.8334 - learning_rate: 1.0000e-04\n",
            "Epoch 20/45\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 580ms/step - accuracy: 0.7485 - loss: 0.8526 - val_accuracy: 0.7667 - val_loss: 0.8482 - learning_rate: 1.0000e-04\n",
            "Epoch 21/45\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 589ms/step - accuracy: 0.7445 - loss: 0.9132 - val_accuracy: 0.8000 - val_loss: 0.8265 - learning_rate: 1.0000e-04\n",
            "Epoch 22/45\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 551ms/step - accuracy: 0.7189 - loss: 0.8409 - val_accuracy: 0.8333 - val_loss: 0.7052 - learning_rate: 1.0000e-04\n",
            "Epoch 23/45\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 609ms/step - accuracy: 0.7040 - loss: 0.8293 - val_accuracy: 0.8000 - val_loss: 0.6779 - learning_rate: 1.0000e-04\n",
            "Epoch 24/45\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 713ms/step - accuracy: 0.8077 - loss: 0.7533 - val_accuracy: 0.8000 - val_loss: 0.5618 - learning_rate: 1.0000e-04\n",
            "Epoch 25/45\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 552ms/step - accuracy: 0.8009 - loss: 0.6324 - val_accuracy: 0.9000 - val_loss: 0.4800 - learning_rate: 1.0000e-04\n",
            "Epoch 26/45\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 553ms/step - accuracy: 0.7716 - loss: 0.6942 - val_accuracy: 0.8333 - val_loss: 0.5536 - learning_rate: 1.0000e-04\n",
            "Epoch 27/45\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 741ms/step - accuracy: 0.7423 - loss: 0.7162 - val_accuracy: 0.9000 - val_loss: 0.4727 - learning_rate: 1.0000e-04\n",
            "Epoch 28/45\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 790ms/step - accuracy: 0.8019 - loss: 0.6649 - val_accuracy: 0.9000 - val_loss: 0.4928 - learning_rate: 1.0000e-04\n",
            "Epoch 29/45\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 543ms/step - accuracy: 0.8766 - loss: 0.4437 - val_accuracy: 0.8667 - val_loss: 0.4280 - learning_rate: 1.0000e-04\n",
            "Epoch 30/45\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 534ms/step - accuracy: 0.8591 - loss: 0.4133 - val_accuracy: 0.9333 - val_loss: 0.3838 - learning_rate: 1.0000e-04\n",
            "Epoch 31/45\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 631ms/step - accuracy: 0.8529 - loss: 0.4408 - val_accuracy: 0.9000 - val_loss: 0.4418 - learning_rate: 1.0000e-04\n",
            "Epoch 32/45\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 671ms/step - accuracy: 0.8158 - loss: 0.5001 - val_accuracy: 0.9333 - val_loss: 0.4014 - learning_rate: 1.0000e-04\n",
            "Epoch 33/45\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 573ms/step - accuracy: 0.9144 - loss: 0.3568 - val_accuracy: 0.9000 - val_loss: 0.3556 - learning_rate: 1.0000e-04\n",
            "Epoch 34/45\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 872ms/step - accuracy: 0.9016 - loss: 0.3748 - val_accuracy: 0.9667 - val_loss: 0.2611 - learning_rate: 1.0000e-04\n",
            "Epoch 35/45\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 586ms/step - accuracy: 0.8701 - loss: 0.4219 - val_accuracy: 0.9333 - val_loss: 0.2836 - learning_rate: 1.0000e-04\n",
            "Epoch 36/45\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 553ms/step - accuracy: 0.8502 - loss: 0.4205 - val_accuracy: 0.9000 - val_loss: 0.2836 - learning_rate: 1.0000e-04\n",
            "Epoch 37/45\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 885ms/step - accuracy: 0.8509 - loss: 0.4195 - val_accuracy: 0.9667 - val_loss: 0.2225 - learning_rate: 1.0000e-04\n",
            "Epoch 38/45\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 555ms/step - accuracy: 0.9354 - loss: 0.2736 - val_accuracy: 0.9333 - val_loss: 0.2551 - learning_rate: 1.0000e-04\n",
            "Epoch 39/45\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 727ms/step - accuracy: 0.9047 - loss: 0.3042 - val_accuracy: 0.9000 - val_loss: 0.3152 - learning_rate: 1.0000e-04\n",
            "Epoch 40/45\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 683ms/step - accuracy: 0.9230 - loss: 0.2972 - val_accuracy: 0.9667 - val_loss: 0.1789 - learning_rate: 1.0000e-04\n",
            "Epoch 41/45\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 565ms/step - accuracy: 0.9177 - loss: 0.2889 - val_accuracy: 0.9667 - val_loss: 0.1958 - learning_rate: 1.0000e-04\n",
            "Epoch 42/45\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 850ms/step - accuracy: 0.9372 - loss: 0.3034 - val_accuracy: 0.9000 - val_loss: 0.2903 - learning_rate: 1.0000e-04\n",
            "Epoch 43/45\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 549ms/step - accuracy: 0.8203 - loss: 0.4528 - val_accuracy: 0.9333 - val_loss: 0.2091 - learning_rate: 1.0000e-04\n",
            "Epoch 44/45\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 874ms/step - accuracy: 0.8968 - loss: 0.3213 - val_accuracy: 0.9667 - val_loss: 0.2263 - learning_rate: 5.0000e-05\n",
            "Epoch 45/45\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 592ms/step - accuracy: 0.9034 - loss: 0.2883 - val_accuracy: 0.9667 - val_loss: 0.2074 - learning_rate: 5.0000e-05\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - accuracy: 0.9569 - loss: 0.1973\n",
            "Validation accuracy: 96.67%\n"
          ]
        }
      ]
    }
  ]
}